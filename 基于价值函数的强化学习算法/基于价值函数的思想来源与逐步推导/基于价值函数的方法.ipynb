{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 从零开始学强化学习系列（三）—— 基于价值函数的方法的初步介绍"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "​ 从这篇文章开始，我们要正式开始学习强化学习的一些基本算法。在我们的强化学习系列的流程图中，我们已经知道强化学习算法大概有三种类别，第一种是**基于价值的算法**，第二种是**基于策略的算法**，第三种则是将前两种方法进行了一个结合，我们将其称之为**基于演员和评论员的算法**。那么在今天的这篇文章中，我们就从基于价值的算法开始介绍，我们直接通过一个例子来大概了解基于价值的算法是怎样的。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 悬崖行走问题"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "​ 悬崖行走问题是强化学习的一个经典问题，如图所示，该问题中智能体需要从$S$点处出发，到达目的地$G$，同时避免掉进悬崖(灰色部分)，每走一步都会得到-1的惩罚，掉进悬崖会有-100的惩罚，但游戏不会结束，智能体会回到出发点，游戏继续，直至到达目的地结束游戏。智能体需要尽快地到达目的地，为了到达目的地，智能体可以沿着例如图中蓝色和红色的线行走。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Alt text](1.jpg \"悬崖行走问题图片\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "​ 在悬崖行走问题的环境中，我们怎么计算状态动作价值呢？我们可以选择一条路线，计算出这条路线上每个状态动作的值。在悬崖行走问题里面，智能体每走一步都会拿到-1分的奖励，只有到达目的地之后，智能体才会停止。\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 如果时间折扣率 $\\gamma=0$,如图a所示，我们考虑的就是单步的奖励，丝毫不在乎未来会带来怎么样的奖励，我们可以认为这是一种目光短浅的计算方法。\n",
    "\n",
    "- 如果时间折扣率 $\\gamma=1$ ,如图b所示，就等于把后续所有的奖励全部加起来，我们可以认为它是目光过于长远的方法。在b图中我们可以观察到红色路径和蓝色路径上的Q值，因此，此时我们再让智能体从第一个位置为选择时，它可以观察到上方的价值为-15，而右边的价值为-11，它知道往右边走的价值更大，所以它会选择往右走。\n",
    "\n",
    "- 如果 $\\gamma=0.6$ ,如图c所示，我们的目光如果没有放得太长远，我们可以利用公式$G_{t}=r_{t+1}+\\gamma G_{t+1}$从前往后推得每一个位置的状态动作价值。 $$ \\begin{aligned}\n",
    "\n",
    "G_{13} & =0 \\\\\n",
    "\n",
    "G_{12} & =r_{13}+\\gamma G_{13} = -1+0.6×0=-1\\\\\n",
    "\n",
    "G_{11} & =r_{12}+\\gamma G_{12} = -1+0.6×(-1)=-1.6\\\\\n",
    "\n",
    "G_{10} & =r_{11}+\\gamma G_{11}=-1+0.6×(-1.6)=-1.96\\\\\n",
    "\n",
    "G_{9} & = r_{10} + \\gamma G_{10} = -1 + 0.6×(-1.96) = -2.176 \\approx -2.18\\\\\n",
    "\n",
    "G_{8} & =r_{9} + \\gamma G_{9} = -1 + 0.6×(-2.176) = -2.3056\\approx -2.3\\\\\n",
    "\n",
    "\\end{aligned} $$\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}