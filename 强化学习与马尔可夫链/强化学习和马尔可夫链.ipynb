{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 强化学习与马尔可夫链\n",
    "### 马尔可夫性\n",
    "&emsp;&emsp;在上一次的文章中我们说到智能体和环境交互时，智能体会基于当前所观测到的状态(observation)去采取动作(action)，即不会收到之前观测的影响。我们将这样一种在随机过程中，一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率仅依赖于当前状态的性质称之为**马尔可夫性**。\n",
    "\n&emsp;&emsp;以离散随机过程为例子，假设随机变量$X_{0},X_{1},...,X_{T}$构成一个随机过程，如果$X_{t+1}$对于过去状态的条件概率分布仅是$X_{t}$的一个函数，则$$p(X_{t+1}=x_{t+1}|X_{0:t}=x_{0:t})=p(X_{t+1}=x_{t+1}|X_{t}=x_{t})$$\n",
    "&emsp;&emsp;所以马尔可夫性质也可以描述为给定当前状态时，将来的状态与过去状态是条件独立的。如果某一过程满足**马尔可夫性质**，那么未来的转移与过去是独立的，它只取决于现在。马尔可夫性质和所有马尔可夫过程的基础。\n",
    "### 马尔可夫链\n",
    "&emsp;&emsp;马尔可夫过程是一组具有马尔可夫性质的随机变量序列$s_{1},...s_{t}$,其中下一个时刻的状态$s_{t+1}$只取决于当前状态$s_{t}$。我们设历史状态为$h_{t}={s_{1},s_{2},...s_{t}}$，即$h_{t}$包含了之前的所有状态，则马尔可夫过程满足条件$$p(s_{t+1}|s_{t})=p(s_{t+1}|h_{t})$$\n",
    "从当前$s_{t}$转移到$s_{t+1}$，它是直接就等于它之前所有的状态转移到$s_{t+1}$。\n",
    "&emsp;&emsp;离散时间的马尔可夫过程也称为**马尔可夫链**。马尔可夫链是最简单的马尔可夫过程，其状态是有限的。例如，图中有4个状态，这4个状态在$s_{1},s_{2},s_{3},s_{4}$之间互相转换。比如从$s_{1}$开始，$s_{1}$有0.1的概率继续留在$s_{1}$状态，有0.2的概率转移到$s_{2}$，有\t0.7的概率转移到$s_{4}$。其它状态均可以此类推。\n",
    "<img src=\"1.jpg\" alt=\"Alt text\" style=\"zoom:50%;\" />\n",
    "### 马尔科夫奖励过程\n",
    "&emsp;&emsp;马尔可夫奖励过程（MRP）是马尔可夫链加上奖励函数。与简单的马尔可夫链相比，马尔可夫奖励过程只是多了一个**奖励函数**(reward function)。奖励函数$R$是一个期望（至于为什么是一个期望后文会解释），表示当我们到达某一个状态的时候,可以获得多大的奖励。其中奖励函数中还包括上一篇博客提到的折扣因子$\\gamma$。\n",
    "#### 回报和价值函数\n",
    "\n",
    "**（从这里开始想强调一下，大家请多留意每个定义的英文原义而不要过多的关注翻译过来的中文，因为不同的参考文献可能对一个单词有不同的翻译。）**\n",
    "&emsp;&emsp;这里我们进一步了解一些概念。**范围(horizon)**是指一个回合的长度（每个回合最大的时间步数），它是由有限个步数组成的。**回报(return)**可以定义为奖励的逐步叠加，假设时刻$t$后的奖励序列为$r_{t+1},r_{t+2},r_{t+3},...,$则回报为$$G_{t}=r_{t+1}+\\gamma r_{t+2}+\\gamma^{2}r_{t+3}+\\gamma^{3}r_{t+4}+...+\\gamma^{T-t-1}r_{T}$$\n",
    "其中，$T$是最终时刻，$\\gamma$是折扣因子，越往后得到的奖励，折扣越多（正如上一次博客提到的，我们一般将折扣因子设置为0.95-0.99中的一个数来平衡未来奖励和当前奖励）。当我们有了回报之后，就可以定义状态的价值了，就是**状态价值函数（state-value-function）。**对于马尔可夫奖励过程，状态价值函数被定义成回报的期望，即\n",
    "$$\\begin{aligned}\n",
    "V^{t}(s) &= \\mathbb{E}[G_{t}|s_{t}=s] \\\\\n",
    "\t\t &= \\mathbb{E}[r_{t+1}+\\gamma r_{t+2}+\\gamma^{2}r_{t+3}+...+\\gamma^{T-t-1}r_{T}|s_{t}=s]\n",
    "\\end{aligned}$$\n",
    "其中，$G_{t}$是之前定义的**折扣回报(discount return)**。在上面的式子中我们对$G_{t}$取了一个期望，学过概率论的同学都知道，期望可以理解为均值，在这里我们在也可以将这个期望理解为从这个状态开始，我们可能获得多大的价值。\n",
    "&emsp;&emsp;那下面我们就通过举一个例子，来阐释在马尔可夫奖励过程中我们应该如何计算价值。例子如图所示，同时我们人为将这个例子中的奖励函数定义为：智能体在进入$s_{1}$的时候会得到5的奖励，进入$s_{7}$的时候会得到10的奖励，进入其它状态都没有奖励。我们可以用向量来表示奖励函数，即$$R=[5,0,0,0,0,0,10]$$\n",
    "&emsp;&emsp;我们随机在该过程中选取4步来采用回报$G$（在该过程为了方便计算将$\\gamma$设置为0.5）。\n",
    "&emsp;&emsp;\t $s_{4},s_{5},s_{6},s_{7}$的回报：0 + 0.5 × 0 + 0.25 × 0 + 0.125 × 10 = 1.25\n",
    "&emsp;&emsp;\t $s_{4},s_{3},s_{2},s_{1}$的回报：0 + 0.5 × 0 + 0.25 × 0 + 0.125 × 5 = 0.625\n",
    "&emsp;&emsp;\t $s_{4},s_{5},s_{6},s_{6}$的回报：0 + 0.5 × 0 + 0.25 × 0 + 0.125 × 0 = 0\n",
    "&emsp;&emsp;\t我们这里对第一条轨迹进行讲解：在$s_{4}$的时候，奖励为0。下一个状态$s_{5}$的时候，因为我们已经到了下一步，所以要把$s_{5}$进行折扣，$s_{5}$的奖励也是0。然后是$s_{6}$,奖励是0,折扣因子是$0.5^{2}$,0.25。到达$s_{7}$后，我们获得了一个奖励，但是因为状态$s_{7}$的奖励是未来才获得的奖励，所以我们要对之进行3次折扣。此时回报就是1.25。其它轨迹的回报也可以像这样计算。\n",
    "![Alt text](2.jpg)\n",
    "&emsp;&emsp;这里就引出了一个问题，当我们有一些轨迹的实际汇报时，我们如何计算状态价值函数呢？比如我们想知道 s4 的价值，即当我们进入 s4 后，它的价值到底如何？\n",
    "&emsp;&emsp;一个可行的做法就是我们可以生成很多轨迹，然后把轨迹都叠加起来。比如我们可以从 s4 开始，采样生成很多轨迹，把这些轨迹的回报都计算出来，然后将其取平均值作为我们进入 s4 的价值。这其实就是一种计算价值函数的办法，也就是通过**蒙特卡洛（MonteCarlo，MC）采样**的方法计算 s4 的价值。\n",
    "#### 贝尔曼方程\n",
    "&emsp;&emsp;刚才提到的通过蒙特卡罗采样的方法来计算价值函数的方法是一种无偏估计，因为我们就是直接对所有真实的轨迹取一个均值。但是这种做法的问题也显然易见，当我们有一些环境需要大量的时间才能完成一次采样时，该做法的成本就会成指数上升。\n",
    "&emsp;&emsp;所以我们介绍另一种做法，从价值函数中推导出**贝尔曼方程(Bellman equation)**:\n",
    "$$V(s)=R(s)+\\gamma\\sum_{s^{\\prime}\\in S}p(s^{\\prime}|s)V(s^{\\prime})$$\n",
    "其中$R_{s}$表示的是即时奖励,$\\gamma\\sum_{s^{\\prime}\\in S}p(s^{\\prime}|s)V(s^{\\prime})$表示的是未来奖励的折扣总和。$s^{\\prime}$可以看成未来的所有状态，$p(s^{\\prime}|s)$是指从当前状态转移到未来状态的概率。$V(s^{\\prime})$代表的是未来某一个状态的价值。我们从当前状态开始，有一定概率去到未来的所有状态，所以我们要把$p(s^{\\prime}|s)$写上去。我们得到未来状态后，得到未来状态后，乘一个$\\gamma$，这样就可以把未来的奖励打折扣。$\\gamma\\sum_{s^{\\prime}\\in S}p(s^{\\prime}|s)V(s^{\\prime})$可以看成未来奖励的总折扣。\n",
    "&emsp;&emsp;贝尔曼方程定义了当前状态与未来状态之间的关系。未来奖励的折扣总和加上即时奖励，就组成了贝尔曼方程。\n",
    "&emsp;&emsp;假设有一个马尔可夫链如下图所示，贝尔曼方程描述的就是当前状态到未来状态的一个转移。如图所示，假设我们当前在 s1，那么它只可能去到 3 个未来的状态：有 0.1 的概率留在它当前位置，有 0.2 的概率去到 s2 状态，有 0.7 的概率去到 s4 状态。所以我们把状态转移概率乘它未来的状态的价值，再加上它的即时奖励（immediate reward），就会得到它当前状态的价值。贝尔曼方程定义的就是当前状态与未来状态的迭代关系。\n",
    "![Alt text](3.jpg)\n",
    "#### 计算马尔可夫奖励过程中价值的方法\n",
    "&emsp;&emsp;第一种方法，我们可以使用**蒙特卡罗方法**来计算价值。蒙特卡罗法就是当得到一个马尔可夫奖励过程后，我们可以从某个状态开始，让智能体“随波逐流”，这样就会产生一个轨迹，产生出一个轨迹之后，就会得到该状态在这个轨迹中的回报。之后我们积累了一定数量的轨迹之后，我们将所有的回报求和得到$G_{t}$,然后拿其除以轨迹数量，就会得到某个状态的价值。\n",
    "&emsp;&emsp;比如我们要计算 $s4$ 状态的价值，可以从 $s4$ 状态开始，随机产生很多轨迹。让智能体“随波逐流”，产生轨迹。每个轨迹都会得到一个回报，我们得到大量的回报，比如 100个、1000 个回报，然后直接取平均值，就可以等价于现在 $s4$ 的价值，因为 $s4$ 的价值 $V (s4)$ 定义了我们未来可能得到多少的奖励。这就是蒙特卡洛采样的方法。\n",
    "&emsp;&emsp;第二种方法，我们可以一直迭代迭代贝尔曼方程，直到价值函数收敛，此时我们就可以得到某个状态的价值。我们通过自举(bootstrapping)的方法不停地迭代贝尔曼方程，当最后更新的状态与上一个状态区别不大的时候，更新就可以停止，我们就可以输出最新的$V^{\\prime}(s)$作为它当前的状态的价值。这里把贝尔曼方程当作一个贝尔曼更新(Bellman update),这样就可以得到状态的价值。**根据其他估算值来更新估算值的思想，我们称其为自举。**\n",
    "### 马尔可夫决策过程\n",
    "&emsp;&emsp;相对于马尔可夫奖励过程，马尔可夫决策过程多了决策（决策是指动作），其他的定义与马尔可夫奖励过程的是类似的。此外，状态转移也多了一个条件，变成了 $p(s_{t+1}=s{\\prime}|s_{t},a_{t})$。未来的状态不仅依赖于当前的状态，也依赖于在当前状态智能体采取的动作。马尔可夫决策过程满足条件：$$p(s_{t+1}|s_{t},a_{t})=p(s_{t+1}|h_{t},a_{t})$$对于奖励函数，它也多了一个当前的动作，变成了 $R(s_{t}=s,a_{t}=a)=\\mathbb{E}[r_{t}|s_{t}=s,a_{t}=a]$。当前的状态以及采取的动作会决定智能体在当前可能得到的奖励多少。\n",
    "\n",
    "#### 马尔可夫决策过程中的策略\n",
    "&emsp;&emsp;策略定义了在某一个状态应该采取什么样的动作。知道动作后，我们可以把当前状态代入策略函数来得到一个概率，即$$\\pi(a|s)=p(a_{t}=a|s_{t}=s)$$概率代表在所有可能的动作里面怎样采取行动，比如可能有 0.7 的概率往左走，有 0.3 的概率往右走，这是一个概率的表示。另外策略也可能是确定的，它有可能直接输出一个值，或者直接告诉我们当前应该采取什么样的动作,而不是一个动作的概率。假设概率函数是平稳的，不同时间点，我们采取的动作其实都是在对策略函数进行采样。\n",
    "&emsp;&emsp;已知马尔可夫决策过程和策略$\\pi$,我们可以把马尔可夫决策过程转换成马尔科夫奖励过程。在马尔可夫决策过程里面，状态转移函数$P(s^{\\prime}|s,a)$基于它当前的状态以及它当前的动作。因为我们现在已知策略函数，也就是已知在每一个状态下，可能采取的动作的概率，所以我们就可以直接把动作进行加和，去掉a，这样我们就可以得到对于马尔可夫奖励过程的转移，这里就没有动作，即$$P_{\\pi}(s^{\\prime}|s)=\\sum_{a\\in A}\\pi(a|s)p(s^{\\prime}|s,a)$$\n",
    "&emsp;&emsp;对于奖励函数，我们也可以用概率加权的方式把动作去掉，这样就会得到类似于马尔可夫奖励过程的奖励函数，即$$r_{\\pi}(s)=\\sum_{a\\in A}\\pi(a|s)R(s,a)$$\n",
    "\n",
    "#### 马尔可夫决策中的价值函数\n",
    "&emsp;&emsp;马尔可夫决策过程中的价值函数可定义为$$V_{\\pi}(s) = \\mathbb{E}_{\\pi}[G_{t}|s_{t}=s]$$其中，期望基于我们采取的策略。当策略决定后，我们通过对策略进行采样来得到一个期望，计算出它的价值函数。\n",
    "&emsp;&emsp;同时，因为我们才马尔可夫决策过程中引入了动作，所以这里我们另外引入了一个**Q函数(Q-function)**。$Q$函数也被称为**动作价值函数(action-value-function)**。$Q$函数定义的是在某一个状态采取某一个动作，它有可能得到的回报的一个期望，即$$Q_{\\pi}(s,a)=\\mathbb{E}_{\\pi}[G_{t}|s_{t},a_{t}=a]$$这里的期望其实也是基于策略函数的。所以我们需要对策略函数进行一个加权，然后得到它的价值。对$Q$函数中的动作进行加和，就可以得到价值函数:$$V_{\\pi}(s)=\\sum_{a\\in A}\\pi(a|s)Q_{\\pi}(s,a)$$\n",
    "\n",
    "#### 预测与控制\n",
    "&emsp;&emsp;预测和控制是马尔可夫决策过程中的核心问题。\n",
    "&emsp;&emsp;预测（评估一个给定的策略）的输入是马尔可夫决策过程<$S,A,P,R,\\gamma$>和策略$\\pi$,输出是价值函数$V_{\\pi}$。预测是指定一个马尔可夫决策过程以及一个策略$\\pi$,计算它的价值函数，也就是计算每个状态的价值。\n",
    "&emsp;&emsp;控制（搜索最佳策略）的输入是马尔可夫决策过程<$S,A,P,R,\\gamma$>，输出是最佳价值函数$V^{*}$和最佳策略$\\pi^{*}$。控制就是我们去寻找一个最佳的策略，然后同时输出它的最佳价值函数和最佳策略。\n",
    "&emsp;&emsp;要强调的是，这两者的区别就在于，预测问题是给定一个策略，我们要确定它的价值函数是多少。而控制问题是在没有策略的前提下，我们要确定最佳的价值函数以及对应的决策方案。实际上，这两者是递进的关系，在强化学习中，我们通过解决预测问题，进而解决控制问题。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
